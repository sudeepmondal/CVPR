# -*- coding: utf-8 -*-
"""Performance_Lab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WNvL6syT_zDw_Ifn9ck8gVV58jOm9rPc
"""

import pandas as pd

df = pd.read_csv('perception.csv')

df.head()

X1 = df['X1'].values
X2 = df['X2'].values
Y = df['Y'].values

import numpy as np
import random
import math
import matplotlib.pyplot as plt
from IPython import display

lr = 0.01
theta = 0.5
max_epochs = 100

W = [random.uniform(-1, 1), random.uniform(-1, 1)]
b = random.uniform(-1, 1)

epoch = 0
count = 0
losses = []

while count < len(Y) and epoch < max_epochs:
    total_error = 0
    for i in range(len(Y)):
        ws = W[0] * X1[i] + W[1] * X2[i] + b

        pred = 0
        if ws > theta:
            pred = 1

        # calculate error
        error = Y[i] - pred
        total_error += abs(error)  # total error

        if error != 0:
            count = 0
            W[0] += lr * error * X1[i]
            W[1] += lr * error * X2[i]
            b += lr * error
        else:
            count += 1

    losses.append(total_error)
    epoch += 1

print(f"Final Weights: W1 = {W[0]}, W2 = {W[1]}, Bias = {b}")

plt.figure(figsize=(6, 4))
plt.plot(range(epoch), losses, label="Training Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Loss Curve (Convergence Behavior)")
plt.legend()
plt.show()

correct_predictions = 0
for i in range(len(Y)):
    ws = W[0] * X1[i] + W[1] * X2[i] + b
    pred = 0
    if ws > theta:
        pred = 1
    if pred == Y[i]:
        correct_predictions += 1

accuracy = correct_predictions / len(Y)
print(f"Accuracy: {accuracy * 100}%")

lr_values = [0.01, 0.5]
for lr in lr_values:
    print(f"\nRunning experiment with lr = {lr}")

    W = [random.uniform(-1, 1), random.uniform(-1, 1)]
    b = random.uniform(-1, 1)

    epoch = 0
    count = 0
    losses = []
    while count < len(Y) and epoch < max_epochs:
        total_error = 0
        for i in range(len(Y)):
            ws = W[0] * X1[i] + W[1] * X2[i] + b
            pred = 0
            if ws > theta:
                pred = 1
            error = Y[i] - pred
            total_error += abs(error)
            if error != 0:
                count = 0
                W[0] += lr * error * X1[i]
                W[1] += lr * error * X2[i]
                b += lr * error
            else:
                count += 1
        losses.append(total_error)
        epoch += 1

    plt.plot(range(epoch), losses, label=f"lr = {lr}")

plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Loss Curve for Different Learning Rates")
plt.legend()
plt.show()

for init_method in ['random', 'uniform']:
    print(f"\nRunning experiment with weight initialization: {init_method}")

    if init_method == 'random':
        W = [random.random(), random.random()]
    elif init_method == 'uniform':
        W = [random.uniform(-1, 1), random.uniform(-1, 1)]

    b = random.uniform(-1, 1)

    epoch = 0
    count = 0
    losses = []
    while count < len(Y) and epoch < max_epochs:
        total_error = 0
        for i in range(len(Y)):
            ws = W[0] * X1[i] + W[1] * X2[i] + b
            pred = 0
            if ws > theta:
                pred = 1
            error = Y[i] - pred
            total_error += abs(error)
            if error != 0:
                count = 0
                W[0] += lr * error * X1[i]
                W[1] += lr * error * X2[i]
                b += lr * error
            else:
                count += 1
        losses.append(total_error)
        epoch += 1

    plt.plot(range(epoch), losses, label=f"{init_method} init")

plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Loss Curve for Different Weight Initializations")
plt.legend()
plt.show()

